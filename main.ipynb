{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from config import *\n",
    "from preprocessing import PREPROCESSING_CONFIGURATIONS\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=Warning)\n",
    "\n",
    "\n",
    "def _generate_config_key(config):\n",
    "    return \" + \".join(func.__name__ for func in config)\n",
    "\n",
    "\n",
    "def _prepare_data(\n",
    "    datapath,\n",
    "    target_label,\n",
    "    columns_to_ignore=None,\n",
    "    labels_to_ignore=None,\n",
    "    test_size=0.2,\n",
    "):\n",
    "    # Read the dataset\n",
    "    df = pd.read_csv(datapath)\n",
    "\n",
    "    # Drop specified columns\n",
    "    if columns_to_ignore:\n",
    "        df.drop(columns=columns_to_ignore, inplace=True)\n",
    "\n",
    "    # Check for columns with all NaN values to also drop them\n",
    "    cols_all_nan = df.columns[df.isna().all()].tolist()\n",
    "    if cols_all_nan:\n",
    "        df.drop(columns=cols_all_nan, inplace=True)\n",
    "        print(f\"Columns {cols_all_nan} were dropped because they contained only NaNs.\")\n",
    "\n",
    "    # Drop rows with invalid categories in the target label\n",
    "    if labels_to_ignore:\n",
    "        df = df[~df[target_label].isin(labels_to_ignore)]\n",
    "\n",
    "    # Check if target variable is categorical and convert to numerical if true\n",
    "    if df[target_label].dtype == \"object\":\n",
    "        le = LabelEncoder()\n",
    "        df[target_label] = le.fit_transform(df[target_label])\n",
    "\n",
    "    # Handle missing values - Option 1: Drop them\n",
    "    # df.dropna(inplace=True)\n",
    "\n",
    "    # Handle missing values - Option 2: Impute them (in this case, with mean)\n",
    "    imputer = SimpleImputer(strategy=\"mean\")\n",
    "    df = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "    # Split features and target variable\n",
    "    X = df.drop(columns=target_label)\n",
    "    y = df[target_label]\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def _apply_techniques(config, X_train, y_train, X_test, y_test):\n",
    "    outliers_detection_technique = config[0]\n",
    "    features_selection_technique = config[1]\n",
    "    oversampling_technique = config[2]\n",
    "\n",
    "    # Apply outlier removal\n",
    "    X_train, y_train = outliers_detection_technique(X_train, y_train)\n",
    "\n",
    "    # Apply features selection\n",
    "    X_train, y_train, X_test, y_test = features_selection_technique(\n",
    "        X_train, y_train, X_test, y_test\n",
    "    )\n",
    "\n",
    "    # Apply oversampling\n",
    "    X_train, y_train = oversampling_technique(X_train, y_train)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def get_data_for_config(\n",
    "    config,\n",
    "    datapath,\n",
    "    target,\n",
    "    columns_to_ignore=None,\n",
    "    labels_to_ignore=None,\n",
    "    test_size=0.2,\n",
    "):\n",
    "    X_train, y_train, X_test, y_test = _prepare_data(\n",
    "        datapath=datapath,\n",
    "        columns_to_ignore=columns_to_ignore,\n",
    "        target_label=target,\n",
    "        labels_to_ignore=labels_to_ignore,\n",
    "        test_size=test_size,\n",
    "    )\n",
    "    X_train, y_train, X_test, y_test = _apply_techniques(\n",
    "        config, X_train, y_train, X_test, y_test\n",
    "    )\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset: student_pred:  73%|███████▎  | 110/150 [00:20<00:08,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_outliers_iqr + features_selection_rfe + oversampling_smote is invalid for dataset student_pred because: Expected n_neighbors <= n_samples,  but n_samples = 8, n_neighbors = 11\n",
      "remove_outliers_iqr + features_selection_rfe + oversampling_svm_smote is invalid for dataset student_pred because: Expected n_neighbors <= n_samples,  but n_samples = 8, n_neighbors = 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset: student_pred:  75%|███████▍  | 112/150 [00:20<00:09,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_outliers_iqr + features_selection_rfe + oversampling_adasyn is invalid for dataset student_pred because: Expected n_neighbors <= n_samples,  but n_samples = 8, n_neighbors = 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset: student_pred:  75%|███████▌  | 113/150 [00:21<00:10,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_outliers_iqr + features_selection_rfe + oversampling_smote_borderline is invalid for dataset student_pred because: Expected n_neighbors <= n_samples,  but n_samples = 8, n_neighbors = 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset: student_pred:  76%|███████▌  | 114/150 [00:21<00:11,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_outliers_iqr + features_selection_rfe + oversampling_nc_smote is invalid for dataset student_pred because: SMOTE-NC is not designed to work only with numerical features. It requires some categorical features.\n",
      "K-nearest-neighbor failed to train with configuration remove_outliers_iqr + features_selection_variance_threshold + oversampling_none because: 'Flags' object has no attribute 'c_contiguous'\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(y_true, y_pred, y_prob):\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"f1\": f1_score(y_true, y_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_true, y_prob),\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_classifiers(X_train, y_train, X_test, y_test, config_key):\n",
    "    results = {}\n",
    "    for classifier_name, classifier_info in classifiers.items():\n",
    "        try:\n",
    "            clf = classifier_info[\"model\"]\n",
    "            clf.fit(X_train, y_train)\n",
    "            predictions = clf.predict(X_test)\n",
    "            if hasattr(clf, \"predict_proba\"):\n",
    "                probabilities = clf.predict_proba(X_test)[\n",
    "                    :, 1\n",
    "                ]  # Probabilities for the positive class\n",
    "            else:\n",
    "                probabilities = predictions  # For models like SVM without predict_proba, use predictions\n",
    "            metrics = compute_metrics(y_test, predictions, probabilities)\n",
    "\n",
    "            results[classifier_name] = {\n",
    "                \"metrics\": metrics,\n",
    "                \"data\": {\n",
    "                    \"X_train\": X_train,\n",
    "                    \"y_train\": y_train,\n",
    "                    \"X_test\": X_test,\n",
    "                    \"y_test\": y_test,\n",
    "                    \"y_pred\": predictions,\n",
    "                },\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"{classifier_name} failed to train with configuration {config_key} because: {e}\"\n",
    "            )\n",
    "    return results\n",
    "\n",
    "\n",
    "classifiers = {\n",
    "    \"RandomForest\": {\n",
    "        \"model\": RandomForestClassifier(),\n",
    "    },\n",
    "    \"K-nearest-neighbor\": {\n",
    "        \"model\": KNeighborsClassifier(),\n",
    "    },\n",
    "    \"Artificial Neural Network\": {\n",
    "        \"model\": MLPClassifier(),\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"model\": DecisionTreeClassifier(),\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        \"model\": LogisticRegression(),\n",
    "    },\n",
    "    \"Support Vector Machine\": {\n",
    "        \"model\": SVC(),  # https: //www.kaggle.com/code/sunayanagawde/ml-algorithms-usage-and-prediction?scriptVersionId=120249289&cellId=62\n",
    "    },\n",
    "    \"Naive Bayes\": {\n",
    "        \"model\": GaussianNB(),\n",
    "    },\n",
    "    \"XG-boost\": {\n",
    "        \"model\": XGBClassifier(),\n",
    "    },\n",
    "}\n",
    "\n",
    "kaggle = (\n",
    "    \"kaggle\",\n",
    "    KAGGLE_DATA_PATH,\n",
    "    KAGGLE_IGNORED_FEAT,\n",
    "    KAGGLE_IGNORED_LABELS,\n",
    "    KAGGLE_TARGET,\n",
    "    0.8,\n",
    ")\n",
    "moodle = (\n",
    "    \"moodle\",\n",
    "    MOODLE_DATA_PATH,\n",
    "    MOODLE_IGNORED_FEAT,\n",
    "    MOODLE_IGNORED_LABELS,\n",
    "    MOODLE_TARGET,\n",
    "    0.2,\n",
    ")\n",
    "student_pred = (\n",
    "    \"student_pred\",\n",
    "    STUDENT_PRED_PATH,\n",
    "    STUDENT_PRED_IGNORED_FEAT,\n",
    "    STUDENT_PRED_IGNORED_LABELS,\n",
    "    STUDENT_PRED_TARGET,\n",
    "    0.2,\n",
    ")\n",
    "datasets = [\n",
    "    kaggle,\n",
    "    moodle,\n",
    "    student_pred,\n",
    "]\n",
    "\n",
    "# Main dictionary to store results\n",
    "all_results = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    dataset_name = dataset[0]\n",
    "    datapath = dataset[1]\n",
    "    columns_to_ignore = dataset[2]\n",
    "    labels_to_ignore = dataset[3]\n",
    "    target_label = dataset[4]\n",
    "    test_size = dataset[5]\n",
    "    for config in tqdm(PREPROCESSING_CONFIGURATIONS, desc=f\"Dataset: {dataset_name}\"):\n",
    "        config_key = _generate_config_key(config)\n",
    "        try:\n",
    "            X_train, y_train, X_test, y_test = get_data_for_config(\n",
    "                config,\n",
    "                datapath=datapath,\n",
    "                target=target_label,\n",
    "                columns_to_ignore=columns_to_ignore,\n",
    "                labels_to_ignore=labels_to_ignore,\n",
    "                test_size=test_size,\n",
    "            )\n",
    "            all_results[config_key] = train_classifiers(\n",
    "                X_train, y_train, X_test, y_test, config_key\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"{config_key} is invalid for dataset {dataset_name} because: {e}\")\n",
    "        # Serialize results\n",
    "        with open(f\"results/all_results_{dataset_name}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(all_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO-DO\n",
    "\n",
    "- implementar cross-validation\n",
    "- ✅ limpiar los NaN de los datasets\n",
    "- ✅ en feat_selection se usa k=10. cambiar esto a tomar los mas relevantes calculando k en funcion de cada escenario Y/O hacer k = min(n_feat, 10) pq hay datasets que tienen menos de 10 feats\n",
    "- añadir modelos: algo de ensemble (boosting y bagging) y red neuronal mas turbia\n",
    "- graficas para los mejores resultados de cada dataset\n",
    "- ✅ añadir roc-auc a las metricas\n",
    "- plotear graficas de aprendizaje (learning curves) o guardar info para generarlas despues para los mejores modelos o algo asi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
