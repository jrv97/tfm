{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import get_all_configurations, PREPROCESSING_CONFIGURATIONS\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from config import KAGGLE_DATA_PATH, KAGGLE_IGNORED_LABELS, KAGGLE_TARGET\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, learning_curve, train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=Warning)\n",
    "\n",
    "\n",
    "def _generate_config_key(config):\n",
    "    return \" + \".join(func.__name__ for func in config)\n",
    "\n",
    "\n",
    "def _prepare_data(\n",
    "    datapath,\n",
    "    target_label,\n",
    "    columns_to_ignore=None,\n",
    "    labels_to_ignore=None,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "):\n",
    "    # Read the dataset\n",
    "    df = pd.read_csv(datapath)\n",
    "\n",
    "    # Drop specified columns\n",
    "    if columns_to_ignore:\n",
    "        df.drop(columns=columns_to_ignore, inplace=True)\n",
    "\n",
    "    # Drop rows with invalid categories in the target label\n",
    "    if labels_to_ignore:\n",
    "        df = df[~df[target_label].isin(labels_to_ignore)]\n",
    "\n",
    "    # Check if target variable is categorical and convert to numerical if true\n",
    "    if df[target_label].dtype == \"object\":\n",
    "        le = LabelEncoder()\n",
    "        df[target_label] = le.fit_transform(df[target_label])\n",
    "\n",
    "    # Split features and target variable\n",
    "    X = df.drop(columns=target_label)\n",
    "    y = df[target_label]\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def _apply_techniques(config, X_train_, y_train, X_test, y_test):\n",
    "    outliers_detection_technique = config[0]\n",
    "    features_selection_technique = config[1]\n",
    "    oversampling_technique = config[2]\n",
    "\n",
    "    # Apply outlier removal\n",
    "    X_train_, y_train = outliers_detection_technique(X_train_, y_train)\n",
    "\n",
    "    # Apply features selection\n",
    "    X_train_, y_train, X_test, y_test = features_selection_technique(\n",
    "        X_train_, y_train, X_test, y_test\n",
    "    )\n",
    "\n",
    "    # Apply oversampling\n",
    "    X_train_, y_train = oversampling_technique(X_train_, y_train)\n",
    "\n",
    "    return X_train_, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def get_data_for_config(config):\n",
    "    X_train, y_train, X_test, y_test = _prepare_data(\n",
    "        KAGGLE_DATA_PATH,\n",
    "        KAGGLE_TARGET,\n",
    "        labels_to_ignore=KAGGLE_IGNORED_LABELS,\n",
    "        test_size=0.8,\n",
    "    )\n",
    "    X_train, y_train, X_test, y_test = _apply_techniques(\n",
    "        config, X_train, y_train, X_test, y_test\n",
    "    )\n",
    "    # print(f\"Size of X_train: {X_train.shape}\")\n",
    "    # print(f\"Size of y_train: {y_train.shape}\")\n",
    "    # print(f\"Size of X_test: {X_test.shape}\")\n",
    "    # print(f\"Size of y_test: {y_test.shape}\")\n",
    "    # # Printing proportions\n",
    "    # train_counts = y_train.value_counts(normalize=True)\n",
    "    # test_counts = y_test.value_counts(normalize=True)\n",
    "\n",
    "    # print(\n",
    "    #     f\"Proportion in y_train (positive:negative): {train_counts.get(1, 0):.2f} : {train_counts.get(0, 0):.2f}\"\n",
    "    # )\n",
    "    # print(\n",
    "    #     f\"Proportion in y_test (positive:negative): {test_counts.get(1, 0):.2f} : {test_counts.get(0, 0):.2f}\"\n",
    "    # )\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/180 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-nearest-neighbor failed to train with configuration remove_outliers_none + features_selection_none + oversampling_none because: 'Flags' object has no attribute 'c_contiguous'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/180 [00:01<03:04,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-nearest-neighbor failed to train with configuration remove_outliers_none + features_selection_none + oversampling_smote because: 'Flags' object has no attribute 'c_contiguous'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/180 [00:02<03:25,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-nearest-neighbor failed to train with configuration remove_outliers_none + features_selection_none + oversampling_svm_smote because: 'Flags' object has no attribute 'c_contiguous'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/180 [00:03<03:27,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-nearest-neighbor failed to train with configuration remove_outliers_none + features_selection_none + oversampling_adasyn because: 'Flags' object has no attribute 'c_contiguous'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/180 [00:04<03:28,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-nearest-neighbor failed to train with configuration remove_outliers_none + features_selection_none + oversampling_smote_borderline because: 'Flags' object has no attribute 'c_contiguous'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 5/180 [00:05<03:29,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_outliers_none + features_selection_none + oversampling_nc_smote is invalid for this dataset because: SMOTE-NC is not designed to work only with numerical features. It requires some categorical features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 10/180 [00:10<03:06,  1.10s/it]"
     ]
    }
   ],
   "source": [
    "# Define the metrics\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"f1\": f1_score(y_true, y_pred),\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_classifiers(X_train, y_train, X_test, y_test, config_key):\n",
    "    results = {}\n",
    "    for classifier_name, classifier_info in classifiers.items():\n",
    "        try:\n",
    "            clf = classifier_info[\"model\"]\n",
    "            clf.fit(X_train, y_train)\n",
    "            predictions = clf.predict(X_test)\n",
    "            metrics = compute_metrics(y_test, predictions)\n",
    "\n",
    "            results[classifier_name] = {\n",
    "                \"metrics\": metrics,\n",
    "                \"data\": {\n",
    "                    \"X_train\": X_train,\n",
    "                    \"y_train\": y_train,\n",
    "                    \"X_test\": X_test,\n",
    "                    \"y_test\": y_test,\n",
    "                    \"y_pred\": predictions,\n",
    "                },\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"{classifier_name} failed to train with configuration {config_key} because: {e}\"\n",
    "            )\n",
    "    return results\n",
    "\n",
    "\n",
    "classifiers = {\n",
    "    \"RandomForest\": {\n",
    "        \"model\": RandomForestClassifier(),\n",
    "    },\n",
    "    \"K-nearest-neighbor\": {\n",
    "        \"model\": KNeighborsClassifier(),\n",
    "    },\n",
    "    \"Artificial Neural Network\": {\n",
    "        \"model\": MLPClassifier(),\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"model\": DecisionTreeClassifier(),\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        \"model\": LogisticRegression(),\n",
    "    },\n",
    "    \"Support Vector Machine\": {\n",
    "        \"model\": SVC(),  # https: //www.kaggle.com/code/sunayanagawde/ml-algorithms-usage-and-prediction?scriptVersionId=120249289&cellId=62\n",
    "    },\n",
    "    \"Naive Bayes\": {\n",
    "        \"model\": GaussianNB(),\n",
    "    },\n",
    "    \"XG-boost\": {\n",
    "        \"model\": XGBClassifier(),\n",
    "    },\n",
    "}\n",
    "\n",
    "# Main dictionary to store results\n",
    "all_results = {}\n",
    "\n",
    "for config in tqdm(PREPROCESSING_CONFIGURATIONS):\n",
    "    try:\n",
    "        config_key = _generate_config_key(config)\n",
    "        X_train, y_train, X_test, y_test = get_data_for_config(config)\n",
    "        all_results[config_key] = train_classifiers(\n",
    "            X_train, y_train, X_test, y_test, config_key\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"{config_key} is invalid for this dataset because: {e}\")\n",
    "\n",
    "# Serialize results\n",
    "with open(f\"all_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
